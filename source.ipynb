{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: streamlit in c:\\users\\palurimanasa\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (1.44.0)\n",
      "Requirement already satisfied: PyPDF2 in c:\\users\\palurimanasa\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (3.0.1)\n",
      "Requirement already satisfied: google-generativeai in c:\\users\\palurimanasa\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (0.8.4)\n",
      "Requirement already satisfied: sentence-transformers in c:\\users\\palurimanasa\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (4.0.1)\n",
      "Requirement already satisfied: jupyter-server-proxy in c:\\users\\palurimanasa\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (4.4.0)\n",
      "Requirement already satisfied: python-dotenv in c:\\users\\palurimanasa\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (1.1.0)\n",
      "Requirement already satisfied: supabase in c:\\users\\palurimanasa\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (2.15.0)\n",
      "Requirement already satisfied: pgvector in c:\\users\\palurimanasa\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (0.4.0)\n",
      "Requirement already satisfied: altair<6,>=4.0 in c:\\users\\palurimanasa\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from streamlit) (5.5.0)\n",
      "Requirement already satisfied: blinker<2,>=1.0.0 in c:\\users\\palurimanasa\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from streamlit) (1.9.0)\n",
      "Requirement already satisfied: cachetools<6,>=4.0 in c:\\users\\palurimanasa\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from streamlit) (5.5.2)\n",
      "Requirement already satisfied: click<9,>=7.0 in c:\\users\\palurimanasa\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from streamlit) (8.1.8)\n",
      "Requirement already satisfied: numpy<3,>=1.23 in c:\\users\\palurimanasa\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from streamlit) (2.2.4)\n",
      "Requirement already satisfied: packaging<25,>=20 in c:\\users\\palurimanasa\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from streamlit) (24.2)\n",
      "Requirement already satisfied: pandas<3,>=1.4.0 in c:\\users\\palurimanasa\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from streamlit) (2.2.3)\n",
      "Requirement already satisfied: pillow<12,>=7.1.0 in c:\\users\\palurimanasa\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from streamlit) (11.1.0)\n",
      "Requirement already satisfied: protobuf<6,>=3.20 in c:\\users\\palurimanasa\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from streamlit) (5.29.4)\n",
      "Requirement already satisfied: pyarrow>=7.0 in c:\\users\\palurimanasa\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from streamlit) (19.0.1)\n",
      "Requirement already satisfied: requests<3,>=2.27 in c:\\users\\palurimanasa\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from streamlit) (2.32.3)\n",
      "Requirement already satisfied: tenacity<10,>=8.1.0 in c:\\users\\palurimanasa\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from streamlit) (9.0.0)\n",
      "Requirement already satisfied: toml<2,>=0.10.1 in c:\\users\\palurimanasa\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from streamlit) (0.10.2)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.4.0 in c:\\users\\palurimanasa\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from streamlit) (4.12.2)\n",
      "Requirement already satisfied: watchdog<7,>=2.1.5 in c:\\users\\palurimanasa\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from streamlit) (6.0.0)\n",
      "Requirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in c:\\users\\palurimanasa\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from streamlit) (3.1.44)\n",
      "Requirement already satisfied: pydeck<1,>=0.8.0b4 in c:\\users\\palurimanasa\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from streamlit) (0.9.1)\n",
      "Requirement already satisfied: tornado<7,>=6.0.3 in c:\\users\\palurimanasa\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from streamlit) (6.4.2)\n",
      "Requirement already satisfied: google-ai-generativelanguage==0.6.15 in c:\\users\\palurimanasa\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from google-generativeai) (0.6.15)\n",
      "Requirement already satisfied: google-api-core in c:\\users\\palurimanasa\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from google-generativeai) (2.24.2)\n",
      "Requirement already satisfied: google-api-python-client in c:\\users\\palurimanasa\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from google-generativeai) (2.166.0)\n",
      "Requirement already satisfied: google-auth>=2.15.0 in c:\\users\\palurimanasa\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from google-generativeai) (2.38.0)\n",
      "Requirement already satisfied: pydantic in c:\\users\\palurimanasa\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from google-generativeai) (2.10.6)\n",
      "Requirement already satisfied: tqdm in c:\\users\\palurimanasa\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from google-generativeai) (4.67.1)\n",
      "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in c:\\users\\palurimanasa\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from google-ai-generativelanguage==0.6.15->google-generativeai) (1.26.1)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in c:\\users\\palurimanasa\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from sentence-transformers) (4.50.2)\n",
      "Requirement already satisfied: torch>=1.11.0 in c:\\users\\palurimanasa\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from sentence-transformers) (2.6.0)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\palurimanasa\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from sentence-transformers) (1.6.1)\n",
      "Requirement already satisfied: scipy in c:\\users\\palurimanasa\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from sentence-transformers) (1.15.2)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in c:\\users\\palurimanasa\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from sentence-transformers) (0.29.3)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\palurimanasa\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jupyter-server-proxy) (3.11.16)\n",
      "Requirement already satisfied: jupyter-server>=1.24.0 in c:\\users\\palurimanasa\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jupyter-server-proxy) (2.15.0)\n",
      "Requirement already satisfied: simpervisor>=1.0.0 in c:\\users\\palurimanasa\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jupyter-server-proxy) (1.0.0)\n",
      "Requirement already satisfied: traitlets>=5.1.0 in c:\\users\\palurimanasa\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jupyter-server-proxy) (5.14.3)\n",
      "Requirement already satisfied: gotrue<3.0.0,>=2.11.0 in c:\\users\\palurimanasa\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from supabase) (2.12.0)\n",
      "Requirement already satisfied: httpx<0.29,>=0.26 in c:\\users\\palurimanasa\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from supabase) (0.28.1)\n",
      "Requirement already satisfied: postgrest<1.1,>0.19 in c:\\users\\palurimanasa\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from supabase) (1.0.1)\n",
      "Requirement already satisfied: realtime<2.5.0,>=2.4.0 in c:\\users\\palurimanasa\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from supabase) (2.4.2)\n",
      "Requirement already satisfied: storage3<0.12,>=0.10 in c:\\users\\palurimanasa\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from supabase) (0.11.3)\n",
      "Requirement already satisfied: supafunc<0.10,>=0.9 in c:\\users\\palurimanasa\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from supabase) (0.9.4)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\palurimanasa\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from altair<6,>=4.0->streamlit) (3.1.6)\n",
      "Requirement already satisfied: jsonschema>=3.0 in c:\\users\\palurimanasa\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from altair<6,>=4.0->streamlit) (4.23.0)\n",
      "Requirement already satisfied: narwhals>=1.14.2 in c:\\users\\palurimanasa\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from altair<6,>=4.0->streamlit) (1.32.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\palurimanasa\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from click<9,>=7.0->streamlit) (0.4.6)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in c:\\users\\palurimanasa\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.12)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in c:\\users\\palurimanasa\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from google-api-core->google-generativeai) (1.69.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\palurimanasa\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from google-auth>=2.15.0->google-generativeai) (0.4.1)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\palurimanasa\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from google-auth>=2.15.0->google-generativeai) (4.9)\n",
      "Requirement already satisfied: pyjwt<3.0.0,>=2.10.1 in c:\\users\\palurimanasa\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from gotrue<3.0.0,>=2.11.0->supabase) (2.10.1)\n",
      "Requirement already satisfied: pytest-mock<4.0.0,>=3.14.0 in c:\\users\\palurimanasa\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from gotrue<3.0.0,>=2.11.0->supabase) (3.14.0)\n",
      "Requirement already satisfied: anyio in c:\\users\\palurimanasa\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from httpx<0.29,>=0.26->supabase) (4.9.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\palurimanasa\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from httpx<0.29,>=0.26->supabase) (2025.1.31)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\palurimanasa\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from httpx<0.29,>=0.26->supabase) (1.0.7)\n",
      "Requirement already satisfied: idna in c:\\users\\palurimanasa\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from httpx<0.29,>=0.26->supabase) (3.10)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\palurimanasa\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from httpcore==1.*->httpx<0.29,>=0.26->supabase) (0.14.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\palurimanasa\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.18.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\palurimanasa\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.3.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\palurimanasa\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.2)\n",
      "Requirement already satisfied: argon2-cffi>=21.1 in c:\\users\\palurimanasa\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jupyter-server>=1.24.0->jupyter-server-proxy) (23.1.0)\n",
      "Requirement already satisfied: jupyter-client>=7.4.4 in c:\\users\\palurimanasa\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jupyter-server>=1.24.0->jupyter-server-proxy) (8.6.3)\n",
      "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in c:\\users\\palurimanasa\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jupyter-server>=1.24.0->jupyter-server-proxy) (5.7.2)\n",
      "Requirement already satisfied: jupyter-events>=0.11.0 in c:\\users\\palurimanasa\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jupyter-server>=1.24.0->jupyter-server-proxy) (0.12.0)\n",
      "Requirement already satisfied: jupyter-server-terminals>=0.4.4 in c:\\users\\palurimanasa\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jupyter-server>=1.24.0->jupyter-server-proxy) (0.5.3)\n",
      "Requirement already satisfied: nbconvert>=6.4.4 in c:\\users\\palurimanasa\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jupyter-server>=1.24.0->jupyter-server-proxy) (7.16.6)\n",
      "Requirement already satisfied: nbformat>=5.3.0 in c:\\users\\palurimanasa\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jupyter-server>=1.24.0->jupyter-server-proxy) (5.10.4)\n",
      "Requirement already satisfied: overrides>=5.0 in c:\\users\\palurimanasa\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jupyter-server>=1.24.0->jupyter-server-proxy) (7.7.0)\n",
      "Requirement already satisfied: prometheus-client>=0.9 in c:\\users\\palurimanasa\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jupyter-server>=1.24.0->jupyter-server-proxy) (0.21.1)\n",
      "Requirement already satisfied: pywinpty>=2.0.1 in c:\\users\\palurimanasa\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jupyter-server>=1.24.0->jupyter-server-proxy) (2.0.15)\n",
      "Requirement already satisfied: pyzmq>=24 in c:\\users\\palurimanasa\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jupyter-server>=1.24.0->jupyter-server-proxy) (26.3.0)\n",
      "Requirement already satisfied: send2trash>=1.8.2 in c:\\users\\palurimanasa\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jupyter-server>=1.24.0->jupyter-server-proxy) (1.8.3)\n",
      "Requirement already satisfied: terminado>=0.8.3 in c:\\users\\palurimanasa\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jupyter-server>=1.24.0->jupyter-server-proxy) (0.18.1)\n",
      "Requirement already satisfied: websocket-client>=1.7 in c:\\users\\palurimanasa\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jupyter-server>=1.24.0->jupyter-server-proxy) (1.8.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\palurimanasa\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas<3,>=1.4.0->streamlit) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\palurimanasa\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas<3,>=1.4.0->streamlit) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\palurimanasa\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas<3,>=1.4.0->streamlit) (2025.1)\n",
      "Requirement already satisfied: deprecation<3.0.0,>=2.1.0 in c:\\users\\palurimanasa\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from postgrest<1.1,>0.19->supabase) (2.1.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\palurimanasa\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pydantic->google-generativeai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in c:\\users\\palurimanasa\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pydantic->google-generativeai) (2.27.2)\n",
      "Requirement already satisfied: websockets<15,>=11 in c:\\users\\palurimanasa\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from realtime<2.5.0,>=2.4.0->supabase) (14.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\palurimanasa\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp->jupyter-server-proxy) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\palurimanasa\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp->jupyter-server-proxy) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\palurimanasa\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp->jupyter-server-proxy) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\palurimanasa\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp->jupyter-server-proxy) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\palurimanasa\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp->jupyter-server-proxy) (6.3.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\palurimanasa\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp->jupyter-server-proxy) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\palurimanasa\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp->jupyter-server-proxy) (1.18.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\palurimanasa\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3,>=2.27->streamlit) (3.4.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\palurimanasa\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3,>=2.27->streamlit) (2.3.0)\n",
      "Requirement already satisfied: strenum<0.5.0,>=0.4.15 in c:\\users\\palurimanasa\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from supafunc<0.10,>=0.9->supabase) (0.4.15)\n",
      "Requirement already satisfied: networkx in c:\\users\\palurimanasa\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.4.2)\n",
      "Requirement already satisfied: setuptools in c:\\users\\palurimanasa\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (78.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\palurimanasa\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\palurimanasa\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\palurimanasa\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\palurimanasa\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\palurimanasa\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.5.3)\n",
      "Requirement already satisfied: httplib2<1.0.0,>=0.19.0 in c:\\users\\palurimanasa\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from google-api-python-client->google-generativeai) (0.22.0)\n",
      "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in c:\\users\\palurimanasa\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from google-api-python-client->google-generativeai) (0.2.0)\n",
      "Requirement already satisfied: uritemplate<5,>=3.0.1 in c:\\users\\palurimanasa\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from google-api-python-client->google-generativeai) (4.1.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\palurimanasa\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\palurimanasa\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\palurimanasa\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from anyio->httpx<0.29,>=0.26->supabase) (1.3.1)\n",
      "Requirement already satisfied: argon2-cffi-bindings in c:\\users\\palurimanasa\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from argon2-cffi>=21.1->jupyter-server>=1.24.0->jupyter-server-proxy) (21.2.0)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in c:\\users\\palurimanasa\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (5.0.2)\n",
      "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in c:\\users\\palurimanasa\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.71.0)\n",
      "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in c:\\users\\palurimanasa\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.71.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in c:\\users\\palurimanasa\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from httplib2<1.0.0,>=0.19.0->google-api-python-client->google-generativeai) (3.2.1)\n",
      "Requirement already satisfied: h2<5,>=3 in c:\\users\\palurimanasa\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from httpx[http2]<0.29,>=0.26->gotrue<3.0.0,>=2.11.0->supabase) (4.2.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\palurimanasa\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jinja2->altair<6,>=4.0->streamlit) (3.0.2)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in c:\\users\\palurimanasa\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (2024.10.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in c:\\users\\palurimanasa\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.36.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in c:\\users\\palurimanasa\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.23.1)\n",
      "Requirement already satisfied: platformdirs>=2.5 in c:\\users\\palurimanasa\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jupyter-core!=5.0.*,>=4.12->jupyter-server>=1.24.0->jupyter-server-proxy) (4.3.6)\n",
      "Requirement already satisfied: pywin32>=300 in c:\\users\\palurimanasa\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jupyter-core!=5.0.*,>=4.12->jupyter-server>=1.24.0->jupyter-server-proxy) (310)\n",
      "Requirement already satisfied: python-json-logger>=2.0.4 in c:\\users\\palurimanasa\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jupyter-events>=0.11.0->jupyter-server>=1.24.0->jupyter-server-proxy) (3.3.0)\n",
      "Requirement already satisfied: rfc3339-validator in c:\\users\\palurimanasa\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jupyter-events>=0.11.0->jupyter-server>=1.24.0->jupyter-server-proxy) (0.1.4)\n",
      "Requirement already satisfied: rfc3986-validator>=0.1.1 in c:\\users\\palurimanasa\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jupyter-events>=0.11.0->jupyter-server>=1.24.0->jupyter-server-proxy) (0.1.1)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\palurimanasa\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nbconvert>=6.4.4->jupyter-server>=1.24.0->jupyter-server-proxy) (4.13.3)\n",
      "Requirement already satisfied: bleach!=5.0.0 in c:\\users\\palurimanasa\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from bleach[css]!=5.0.0->nbconvert>=6.4.4->jupyter-server>=1.24.0->jupyter-server-proxy) (6.2.0)\n",
      "Requirement already satisfied: defusedxml in c:\\users\\palurimanasa\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nbconvert>=6.4.4->jupyter-server>=1.24.0->jupyter-server-proxy) (0.7.1)\n",
      "Requirement already satisfied: jupyterlab-pygments in c:\\users\\palurimanasa\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nbconvert>=6.4.4->jupyter-server>=1.24.0->jupyter-server-proxy) (0.3.0)\n",
      "Requirement already satisfied: mistune<4,>=2.0.3 in c:\\users\\palurimanasa\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nbconvert>=6.4.4->jupyter-server>=1.24.0->jupyter-server-proxy) (3.1.2)\n",
      "Requirement already satisfied: nbclient>=0.5.0 in c:\\users\\palurimanasa\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nbconvert>=6.4.4->jupyter-server>=1.24.0->jupyter-server-proxy) (0.10.2)\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in c:\\users\\palurimanasa\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nbconvert>=6.4.4->jupyter-server>=1.24.0->jupyter-server-proxy) (1.5.1)\n",
      "Requirement already satisfied: pygments>=2.4.1 in c:\\users\\palurimanasa\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nbconvert>=6.4.4->jupyter-server>=1.24.0->jupyter-server-proxy) (2.19.1)\n",
      "Requirement already satisfied: fastjsonschema>=2.15 in c:\\users\\palurimanasa\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nbformat>=5.3.0->jupyter-server>=1.24.0->jupyter-server-proxy) (2.21.1)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in c:\\users\\palurimanasa\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth>=2.15.0->google-generativeai) (0.6.1)\n",
      "Requirement already satisfied: pytest>=6.2.5 in c:\\users\\palurimanasa\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pytest-mock<4.0.0,>=3.14.0->gotrue<3.0.0,>=2.11.0->supabase) (8.3.5)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\palurimanasa\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from python-dateutil>=2.8.2->pandas<3,>=1.4.0->streamlit) (1.17.0)\n",
      "Requirement already satisfied: webencodings in c:\\users\\palurimanasa\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from bleach!=5.0.0->bleach[css]!=5.0.0->nbconvert>=6.4.4->jupyter-server>=1.24.0->jupyter-server-proxy) (0.5.1)\n",
      "Requirement already satisfied: tinycss2<1.5,>=1.1.0 in c:\\users\\palurimanasa\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from bleach[css]!=5.0.0->nbconvert>=6.4.4->jupyter-server>=1.24.0->jupyter-server-proxy) (1.4.0)\n",
      "Requirement already satisfied: hyperframe<7,>=6.1 in c:\\users\\palurimanasa\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from h2<5,>=3->httpx[http2]<0.29,>=0.26->gotrue<3.0.0,>=2.11.0->supabase) (6.1.0)\n",
      "Requirement already satisfied: hpack<5,>=4.1 in c:\\users\\palurimanasa\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from h2<5,>=3->httpx[http2]<0.29,>=0.26->gotrue<3.0.0,>=2.11.0->supabase) (4.1.0)\n",
      "Requirement already satisfied: fqdn in c:\\users\\palurimanasa\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server>=1.24.0->jupyter-server-proxy) (1.5.1)\n",
      "Requirement already satisfied: isoduration in c:\\users\\palurimanasa\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server>=1.24.0->jupyter-server-proxy) (20.11.0)\n",
      "Requirement already satisfied: jsonpointer>1.13 in c:\\users\\palurimanasa\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server>=1.24.0->jupyter-server-proxy) (3.0.0)\n",
      "Requirement already satisfied: uri-template in c:\\users\\palurimanasa\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server>=1.24.0->jupyter-server-proxy) (1.3.0)\n",
      "Requirement already satisfied: webcolors>=24.6.0 in c:\\users\\palurimanasa\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server>=1.24.0->jupyter-server-proxy) (24.11.1)\n",
      "Requirement already satisfied: iniconfig in c:\\users\\palurimanasa\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pytest>=6.2.5->pytest-mock<4.0.0,>=3.14.0->gotrue<3.0.0,>=2.11.0->supabase) (2.1.0)\n",
      "Requirement already satisfied: pluggy<2,>=1.5 in c:\\users\\palurimanasa\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pytest>=6.2.5->pytest-mock<4.0.0,>=3.14.0->gotrue<3.0.0,>=2.11.0->supabase) (1.5.0)\n",
      "Requirement already satisfied: cffi>=1.0.1 in c:\\users\\palurimanasa\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from argon2-cffi-bindings->argon2-cffi>=21.1->jupyter-server>=1.24.0->jupyter-server-proxy) (1.17.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\palurimanasa\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from beautifulsoup4->nbconvert>=6.4.4->jupyter-server>=1.24.0->jupyter-server-proxy) (2.6)\n",
      "Requirement already satisfied: pycparser in c:\\users\\palurimanasa\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi>=21.1->jupyter-server>=1.24.0->jupyter-server-proxy) (2.22)\n",
      "Requirement already satisfied: arrow>=0.15.0 in c:\\users\\palurimanasa\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from isoduration->jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server>=1.24.0->jupyter-server-proxy) (1.3.0)\n",
      "Requirement already satisfied: types-python-dateutil>=2.8.10 in c:\\users\\palurimanasa\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from arrow>=0.15.0->isoduration->jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server>=1.24.0->jupyter-server-proxy) (2.9.0.20241206)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install streamlit PyPDF2 google-generativeai sentence-transformers jupyter-server-proxy python-dotenv supabase pgvector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting app.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile app.py\n",
    "import streamlit as st\n",
    "import os, tempfile, uuid, time, json, base64, urllib.parse\n",
    "import PyPDF2\n",
    "import google.generativeai as genai\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from dotenv import load_dotenv\n",
    "from supabase import create_client\n",
    "import numpy as np\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Load env variables and initialize clients\n",
    "load_dotenv()\n",
    "GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\")\n",
    "supabase = create_client(os.getenv(\"SUPABASE_URL\"), os.getenv(\"SUPABASE_KEY\"))\n",
    "\n",
    "# Configure Gemini and page\n",
    "genai.configure(api_key=GOOGLE_API_KEY)\n",
    "model = genai.GenerativeModel('gemini-1.5-flash-latest')\n",
    "st.set_page_config(page_title=\"Document & Web Q&A with Gemini\", layout=\"wide\")\n",
    "\n",
    "# Initialize session state variables\n",
    "for key, default in {\n",
    "    'chat_history': [], 'text_chunks': [], 'document_loaded': False,\n",
    "    'document_name': None, 'document_type': None, 'embedder': None,\n",
    "    'document_tag': None, 'pdf_content': None, 'all_chat_histories': {}\n",
    "}.items():\n",
    "    if key not in st.session_state:\n",
    "        st.session_state[key] = default\n",
    "\n",
    "if not GOOGLE_API_KEY:\n",
    "    st.warning(\"Please set the GOOGLE_API_KEY in Streamlit secrets or environment variables!\")\n",
    "\n",
    "# Load sentence transformer model\n",
    "@st.cache_resource\n",
    "def load_sentence_transformer():\n",
    "    return SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# Extract text from PDF\n",
    "def extract_text_from_pdf(pdf_file):\n",
    "    pdf_data = pdf_file.read()\n",
    "    st.session_state.pdf_content = pdf_data\n",
    "    \n",
    "    with tempfile.NamedTemporaryFile(delete=False) as temp_file:\n",
    "        temp_file.write(pdf_data)\n",
    "        temp_file_path = temp_file.name\n",
    "    \n",
    "    text_content = \"\"\n",
    "    with open(temp_file_path, 'rb') as file:\n",
    "        pdf_reader = PyPDF2.PdfReader(file)\n",
    "        for page in pdf_reader.pages:\n",
    "            text_content += page.extract_text() + \"\\n\\n\"\n",
    "    \n",
    "    os.unlink(temp_file_path)\n",
    "    return text_content\n",
    "\n",
    "# URL validation and cleaning\n",
    "def is_valid_url(url):\n",
    "    try:\n",
    "        result = urllib.parse.urlparse(url)\n",
    "        return all([result.scheme, result.netloc])\n",
    "    except ValueError:\n",
    "        return False\n",
    "\n",
    "def clean_url(url):\n",
    "    return url if url.startswith(('http://', 'https://')) else 'https://' + url\n",
    "\n",
    "# Web scraping functions\n",
    "def scrape_url(url):\n",
    "    try:\n",
    "        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'}\n",
    "        response = requests.get(url, headers=headers, timeout=10)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        for script_or_style in soup([\"script\", \"style\"]):\n",
    "            script_or_style.decompose()\n",
    "            \n",
    "        text = soup.get_text()\n",
    "        lines = (line.strip() for line in text.splitlines())\n",
    "        chunks = (phrase.strip() for line in lines for phrase in line.split(\"  \"))\n",
    "        return '\\n'.join(chunk for chunk in chunks if chunk)\n",
    "    except Exception as e:\n",
    "        st.error(f\"Error scraping URL {url}: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def scrape_domain(domain, max_pages=10):\n",
    "    domain = clean_url(domain)\n",
    "    \n",
    "    if not is_valid_url(domain):\n",
    "        st.error(\"Invalid URL. Please enter a valid URL with format: example.com or https://example.com\")\n",
    "        return None\n",
    "        \n",
    "    visited, to_visit = set(), [domain]\n",
    "    all_text = \"\"\n",
    "    \n",
    "    with st.spinner(f\"Scraping website (0/{max_pages} pages)...\"):\n",
    "        progress_bar = st.progress(0)\n",
    "        page_count = 0\n",
    "        \n",
    "        while to_visit and page_count < max_pages:\n",
    "            current_url = to_visit.pop(0)\n",
    "            if current_url in visited:\n",
    "                continue\n",
    "                \n",
    "            try:\n",
    "                # Update progress\n",
    "                page_count += 1\n",
    "                progress_bar.progress(page_count / max_pages)\n",
    "                \n",
    "                # Get and process page\n",
    "                headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'}\n",
    "                response = requests.get(current_url, headers=headers, timeout=10)\n",
    "                \n",
    "                if response.status_code == 200:\n",
    "                    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "                    \n",
    "                    # Clean and extract text\n",
    "                    for script_or_style in soup([\"script\", \"style\"]):\n",
    "                        script_or_style.decompose()\n",
    "                        \n",
    "                    page_text = clean_text_for_storage(soup.get_text())\n",
    "                    all_text += page_text + \"\\n\\n--- Next Page ---\\n\\n\"\n",
    "                    \n",
    "                    # Find new links within same domain\n",
    "                    domain_parts = urllib.parse.urlparse(domain).netloc\n",
    "                    for link in soup.find_all('a', href=True):\n",
    "                        href = link['href']\n",
    "                        full_url = urllib.parse.urljoin(domain, href) if href.startswith('/') else href\n",
    "                        \n",
    "                        if (is_valid_url(full_url) and domain_parts in full_url \n",
    "                            and full_url not in visited and full_url not in to_visit):\n",
    "                            to_visit.append(full_url)\n",
    "                            \n",
    "                visited.add(current_url)\n",
    "                time.sleep(1)  # Be polite to servers\n",
    "                \n",
    "            except Exception as e:\n",
    "                st.warning(f\"Error processing {current_url}: {str(e)}\")\n",
    "                visited.add(current_url)\n",
    "                \n",
    "        progress_bar.progress(1.0)\n",
    "        \n",
    "    return all_text\n",
    "\n",
    "# Text processing\n",
    "def clean_text_for_storage(text):\n",
    "    if text is None:\n",
    "        return \"\"\n",
    "        \n",
    "    # Replace null bytes and filter control characters\n",
    "    text = text.replace('\\x00', '')\n",
    "    cleaned_text = ''.join(char for char in text if ord(char) >= 32 or char in '\\n\\r\\t')\n",
    "    \n",
    "    # Ensure UTF-8 compatibility\n",
    "    return cleaned_text.encode('utf-8', errors='ignore').decode('utf-8')\n",
    "\n",
    "def split_text_into_chunks(text, chunk_size=1500, overlap=300):\n",
    "    chunks = []\n",
    "    for i in range(0, len(text), chunk_size - overlap):\n",
    "        chunk = text[i:i + chunk_size]\n",
    "        if len(chunk) > 100:  # Only add substantial chunks\n",
    "            chunks.append(chunk)\n",
    "    return chunks\n",
    "\n",
    "# Document management\n",
    "def generate_document_tag():\n",
    "    timestamp = int(time.time())\n",
    "    random_id = str(uuid.uuid4())[:8]\n",
    "    return f\"doc_{timestamp}_{random_id}\"\n",
    "\n",
    "# Database operations\n",
    "def save_chat_history_to_database(document_tag, chat_history):\n",
    "    try:\n",
    "        chat_history_json = json.dumps(chat_history)\n",
    "        response = supabase.table(\"chat_histories\").select(\"id\").eq(\"document_tag\", document_tag).execute()\n",
    "        \n",
    "        if response.data:\n",
    "            # Update existing record\n",
    "            record_id = response.data[0][\"id\"]\n",
    "            supabase.table(\"chat_histories\").update({\n",
    "                \"chat_history\": chat_history_json,\n",
    "                \"updated_at\": time.strftime('%Y-%m-%d %H:%M:%S')\n",
    "            }).eq(\"id\", record_id).execute()\n",
    "        else:\n",
    "            # Create new record\n",
    "            supabase.table(\"chat_histories\").insert({\n",
    "                \"document_tag\": document_tag,\n",
    "                \"chat_history\": chat_history_json,\n",
    "                \"created_at\": time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "                \"updated_at\": time.strftime('%Y-%m-%d %H:%M:%S')\n",
    "            }).execute()\n",
    "            \n",
    "        return True\n",
    "    except Exception as e:\n",
    "        st.warning(f\"Error saving chat history: {str(e)}\")\n",
    "        return False\n",
    "\n",
    "def load_chat_history_from_database(document_tag):\n",
    "    try:\n",
    "        response = supabase.table(\"chat_histories\").select(\"chat_history\").eq(\"document_tag\", document_tag).execute()\n",
    "        \n",
    "        if response.data and response.data[0][\"chat_history\"]:\n",
    "            return json.loads(response.data[0][\"chat_history\"])\n",
    "        return []\n",
    "    except Exception as e:\n",
    "        st.warning(f\"Error loading chat history: {str(e)}\")\n",
    "        return []\n",
    "\n",
    "def store_embeddings_in_supabase(text_chunks, embedder, source_type, source_name):\n",
    "    with st.spinner(f\"Generating embeddings for {len(text_chunks)} chunks...\"):\n",
    "        document_tag = generate_document_tag()\n",
    "        st.session_state.document_tag = document_tag\n",
    "        \n",
    "        clean_chunks = [clean_text_for_storage(chunk) for chunk in text_chunks]\n",
    "        embeddings = embedder.encode(clean_chunks)\n",
    "        \n",
    "        with st.spinner(\"Storing embeddings in database...\"):\n",
    "            batch_size = 50\n",
    "            for i in range(0, len(clean_chunks), batch_size):\n",
    "                batch_chunks = clean_chunks[i:i+batch_size]\n",
    "                batch_embeddings = embeddings[i:i+batch_size].tolist()\n",
    "                \n",
    "                # Include document tag in chunk text\n",
    "                batch_data = [\n",
    "                    {\n",
    "                        \"chunk\": f\"[{source_type}: {source_name}] [tag: {document_tag}]\\n{chunk}\",\n",
    "                        \"embedding\": embedding,\n",
    "                    }\n",
    "                    for chunk, embedding in zip(batch_chunks, batch_embeddings)\n",
    "                ]\n",
    "                \n",
    "                try:\n",
    "                    supabase.table(\"text_chunks\").insert(batch_data).execute()\n",
    "                except Exception as e:\n",
    "                    st.error(f\"Error storing embeddings: {str(e)}\")\n",
    "                    return False\n",
    "                    \n",
    "    return True\n",
    "\n",
    "def query_supabase(question, embedder, document_tag, top_k=6):\n",
    "    # Generate embedding for the question\n",
    "    question_embedding = embedder.encode([question])[0]\n",
    "    \n",
    "    # Retrieve chunks from Supabase\n",
    "    response = supabase.table(\"text_chunks\").select(\"*\").execute()\n",
    "    \n",
    "    if not response.data:\n",
    "        return \"No documents found in the database.\"\n",
    "        \n",
    "    # Calculate similarities for chunks with matching document tag\n",
    "    similarities = []\n",
    "    for chunk in response.data:\n",
    "        try:\n",
    "            # Check if chunk belongs to our document\n",
    "            chunk_text = chunk[\"chunk\"]\n",
    "            if f\"[tag: {document_tag}]\" not in chunk_text:\n",
    "                continue\n",
    "                \n",
    "            # Parse embedding properly\n",
    "            chunk_embedding = chunk[\"embedding\"]\n",
    "            if isinstance(chunk_embedding, str):\n",
    "                try:\n",
    "                    chunk_embedding = json.loads(chunk_embedding)\n",
    "                except json.JSONDecodeError:\n",
    "                    continue\n",
    "                    \n",
    "            # Calculate cosine similarity\n",
    "            chunk_embedding = np.array(chunk_embedding)\n",
    "            question_embedding_np = np.array(question_embedding)\n",
    "            \n",
    "            norm_chunk = np.linalg.norm(chunk_embedding)\n",
    "            norm_question = np.linalg.norm(question_embedding_np)\n",
    "            \n",
    "            if norm_chunk > 0 and norm_question > 0:\n",
    "                similarity = np.dot(chunk_embedding, question_embedding_np) / (norm_chunk * norm_question)\n",
    "                similarities.append((chunk[\"chunk\"], similarity))\n",
    "                \n",
    "        except Exception:\n",
    "            continue\n",
    "            \n",
    "    # Sort by similarity\n",
    "    similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # Get top_k chunks\n",
    "    top_chunks = [chunk for chunk, _ in similarities[:top_k]]\n",
    "    \n",
    "    if top_chunks:\n",
    "        cleaned_chunks = [chunk.replace(f\"[tag: {document_tag}]\", \"\").strip() for chunk in top_chunks]\n",
    "        context = \"\\n\\n\".join(cleaned_chunks)\n",
    "        return context\n",
    "    else:\n",
    "        return \"Could not find relevant information in the document.\"\n",
    "\n",
    "def get_gemini_response(question, context):\n",
    "    prompt = f\"\"\"\n",
    "    You are a helpful assistant answering questions based on the provided document or web content.\n",
    "\n",
    "    Context:\n",
    "    {context}\n",
    "\n",
    "    User question: {question}\n",
    "\n",
    "    Instructions:\n",
    "    1. Answer based ONLY on the information in the context above\n",
    "    2. If the context doesn't contain information to answer the question, respond with:\n",
    "       \"The document doesn't contain information to answer this question.\"\n",
    "    3. Be concise but complete in your answer\n",
    "    4. You may cite specific parts of the document to support your answer\n",
    "    5. If the context contains source information in [PDF: filename] or [web: URL] format, you can \n",
    "       mention which source the information came from\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        response = model.generate_content(prompt)\n",
    "        return response.text\n",
    "    except Exception as e:\n",
    "        return f\"Error generating response: {str(e)}. Please try a different question or check your API key.\"\n",
    "\n",
    "def extract_document_tags_from_database():\n",
    "    try:\n",
    "        response = supabase.table(\"text_chunks\").select(\"chunk\").execute()\n",
    "        if not response.data:\n",
    "            return []\n",
    "            \n",
    "        import re\n",
    "        tags = set()\n",
    "        for item in response.data:\n",
    "            chunk = item.get(\"chunk\", \"\")\n",
    "            tag_match = re.search(r'\\[tag: (doc_\\d+_[a-f0-9]+)\\]', chunk)\n",
    "            if tag_match:\n",
    "                tags.add(tag_match.group(1))\n",
    "                \n",
    "        return list(tags)\n",
    "    except Exception as e:\n",
    "        st.warning(f\"Error extracting document tags: {str(e)}\")\n",
    "        return []\n",
    "\n",
    "def extract_source_info_from_tag(tag):\n",
    "    try:\n",
    "        response = supabase.table(\"text_chunks\").select(\"chunk\").execute()\n",
    "        for item in response.data:\n",
    "            chunk = item.get(\"chunk\", \"\")\n",
    "            if f\"[tag: {tag}]\" in chunk:\n",
    "                import re\n",
    "                source_match = re.search(r'\\[(pdf|web): ([^\\]]+)\\]', chunk, re.IGNORECASE)\n",
    "                if source_match:\n",
    "                    return {\n",
    "                        \"source_type\": source_match.group(1).lower(),\n",
    "                        \"source_name\": source_match.group(2)\n",
    "                    }\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        st.warning(f\"Error extracting source info: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def clear_current_document_data():\n",
    "    for key in ['chat_history', 'text_chunks', 'document_loaded', 'document_name', \n",
    "                'document_type', 'document_tag', 'pdf_content']:\n",
    "        st.session_state[key] = [] if key == 'chat_history' or key == 'text_chunks' else None\n",
    "    st.session_state.document_loaded = False\n",
    "\n",
    "# Database setup\n",
    "def ensure_chat_histories_table_exists():\n",
    "    try:\n",
    "        supabase.table(\"chat_histories\").select(\"id\").limit(1).execute()\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        if \"relation\" in str(e) and \"does not exist\" in str(e):\n",
    "            st.error(\"Please create a chat_histories table in your Supabase database with columns: id, document_tag, chat_history, created_at, updated_at\")\n",
    "            return False\n",
    "        else:\n",
    "            st.warning(f\"Error checking chat_histories table: {str(e)}\")\n",
    "            return False\n",
    "\n",
    "# Check database tables\n",
    "ensure_chat_histories_table_exists()\n",
    "\n",
    "# Load embedding model\n",
    "if st.session_state.embedder is None:\n",
    "    with st.spinner(\"Loading embedding model...\"):\n",
    "        st.session_state.embedder = load_sentence_transformer()\n",
    "\n",
    "with st.sidebar:\n",
    "    st.header(\"Upload or Enter URL\")\n",
    "    \n",
    "    input_method = st.radio(\"Choose input method:\", [\"PDF Upload\", \"Website URL\"])\n",
    "    \n",
    "    if input_method == \"PDF Upload\":\n",
    "        uploaded_file = st.file_uploader(\"Choose a PDF file\", type=['pdf'])\n",
    "        \n",
    "        if uploaded_file is not None and (not st.session_state.document_loaded or \n",
    "                                         st.session_state.document_type != \"pdf\" or \n",
    "                                         st.session_state.document_name != uploaded_file.name):\n",
    "            if st.button(\"Process PDF\"):\n",
    "                with st.spinner(\"Processing PDF...\"):\n",
    "                    text_content = extract_text_from_pdf(uploaded_file)\n",
    "                    chunks = split_text_into_chunks(text_content)\n",
    "                    st.info(f\"Document processed: {len(chunks)} text chunks extracted\")\n",
    "                    \n",
    "                    clear_current_document_data()\n",
    "                    \n",
    "                    if store_embeddings_in_supabase(chunks, st.session_state.embedder, \"pdf\", uploaded_file.name):\n",
    "                        st.session_state.text_chunks = chunks\n",
    "                        st.session_state.document_loaded = True\n",
    "                        st.session_state.document_type = \"pdf\"\n",
    "                        st.session_state.document_name = uploaded_file.name\n",
    "                        st.success(f\"Successfully uploaded and indexed: {uploaded_file.name}\")\n",
    "    \n",
    "    else:  # Website URL\n",
    "        url_input = st.text_input(\"Enter website URL or domain:\", placeholder=\"example.com or https://example.com\")\n",
    "        max_pages = st.slider(\"Maximum pages to scrape:\", min_value=1, max_value=20, value=5)\n",
    "        \n",
    "        if url_input and st.button(\"Scrape Website\"):\n",
    "            if not is_valid_url(clean_url(url_input)):\n",
    "                st.error(\"Invalid URL. Please enter a valid domain or URL.\")\n",
    "            else:\n",
    "                with st.spinner(\"Scraping website...\"):\n",
    "                    scraped_text = scrape_domain(url_input, max_pages=max_pages)\n",
    "                    \n",
    "                    if scraped_text:\n",
    "                        chunks = split_text_into_chunks(scraped_text)\n",
    "                        st.info(f\"Website scraped: {len(chunks)} text chunks extracted\")\n",
    "                        \n",
    "                        clear_current_document_data()\n",
    "                        \n",
    "                        if store_embeddings_in_supabase(chunks, st.session_state.embedder, \"web\", url_input):\n",
    "                            st.session_state.text_chunks = chunks\n",
    "                            st.session_state.document_loaded = True\n",
    "                            st.session_state.document_type = \"web\"\n",
    "                            st.session_state.document_name = url_input\n",
    "                            st.success(f\"Successfully scraped and indexed: {url_input}\")\n",
    "    \n",
    "    # Clear button\n",
    "    if st.session_state.document_loaded:\n",
    "        st.success(f\"Current source: {st.session_state.document_name} ({st.session_state.document_type})\")\n",
    "        if st.button(\"Clear Current Source\"):\n",
    "            if st.session_state.chat_history:\n",
    "                save_chat_history_to_database(st.session_state.document_tag, st.session_state.chat_history)\n",
    "            clear_current_document_data()\n",
    "            st.rerun()\n",
    "    \n",
    "    # Document history\n",
    "    st.header(\"Document History\")\n",
    "    try:\n",
    "        document_tags = extract_document_tags_from_database()\n",
    "        \n",
    "        if document_tags:\n",
    "            st.write(\"Previously processed documents:\")\n",
    "            for tag in document_tags:\n",
    "                source_info = extract_source_info_from_tag(tag)\n",
    "                if source_info:\n",
    "                    source_name = source_info[\"source_name\"]\n",
    "                    source_type = source_info[\"source_type\"]\n",
    "                    \n",
    "                    col1, col2 = st.columns([3, 1])\n",
    "                    with col1:\n",
    "                        st.write(f\"{source_name} ({source_type})\")\n",
    "                    with col2:\n",
    "                        if st.button(\"Load\", key=f\"load_{tag}\"):\n",
    "                            # Save current chat history before switching\n",
    "                            if st.session_state.document_loaded and st.session_state.document_tag and st.session_state.chat_history:\n",
    "                                save_chat_history_to_database(st.session_state.document_tag, st.session_state.chat_history)\n",
    "                            \n",
    "                            # Load new document\n",
    "                            st.session_state.document_tag = tag\n",
    "                            st.session_state.document_name = source_name\n",
    "                            st.session_state.document_type = source_type\n",
    "                            st.session_state.document_loaded = True\n",
    "                            \n",
    "                            # Load existing chat history\n",
    "                            chat_history = load_chat_history_from_database(tag)\n",
    "                            st.session_state.chat_history = chat_history if chat_history else []\n",
    "                            st.session_state.pdf_content = None\n",
    "                            \n",
    "                            st.rerun()\n",
    "        else:\n",
    "            st.write(\"No documents processed yet.\")\n",
    "    except Exception as e:\n",
    "        st.warning(f\"Could not retrieve document history: {str(e)}\")\n",
    "\n",
    "# ----- MAIN CHAT UI -----\n",
    "if st.session_state.document_loaded:\n",
    "    # Display document info and PDF viewer if applicable\n",
    "    if st.session_state.document_type == \"pdf\":\n",
    "        col1, col2 = st.columns([6, 2])\n",
    "        with col1:\n",
    "            st.subheader(f\"Chatting with: {st.session_state.document_name}\")\n",
    "        with col2:\n",
    "            if st.session_state.pdf_content:\n",
    "                b64_pdf = base64.b64encode(st.session_state.pdf_content).decode()\n",
    "                pdf_display = f'<a href=\"data:application/pdf;base64,{b64_pdf}\" target=\"_blank\"><button style=\"background-color:#4CAF50;color:white;padding:8px 16px;border:none;border-radius:4px;cursor:pointer;\">View PDF in New Tab</button></a>'\n",
    "                st.markdown(pdf_display, unsafe_allow_html=True)\n",
    "    else:\n",
    "        st.subheader(f\"Chatting with: {st.session_state.document_name}\")\n",
    "    \n",
    "    # Chat history display\n",
    "    chat_container = st.container(height=400, border=True)\n",
    "    with chat_container:\n",
    "        for message in st.session_state.chat_history:\n",
    "            with st.chat_message(message[\"role\"]):\n",
    "                st.write(message[\"content\"])\n",
    "    \n",
    "    # Chat history download\n",
    "    if st.session_state.chat_history:\n",
    "        chat_text = \"\\n\\n\".join([f\"{msg['role'].upper()}: {msg['content']}\" for msg in st.session_state.chat_history])\n",
    "        file_name = f\"chat_history_{st.session_state.document_name.replace(' ', '_')}.txt\"\n",
    "        \n",
    "        st.download_button(\n",
    "            label=\"Download Chat History\",\n",
    "            data=chat_text.encode(),\n",
    "            file_name=file_name,\n",
    "            mime=\"text/plain\",\n",
    "        )\n",
    "    \n",
    "    # Chat input and response handling\n",
    "    user_question = st.chat_input(f\"Ask a question about {st.session_state.document_type}: {st.session_state.document_name}\")\n",
    "    if user_question:\n",
    "        # Add user message to chat history\n",
    "        st.session_state.chat_history.append({\"role\": \"user\", \"content\": user_question})\n",
    "        \n",
    "        # Display user message\n",
    "        with st.chat_message(\"user\"):\n",
    "            st.write(user_question)\n",
    "        \n",
    "        # Generate response\n",
    "        with st.chat_message(\"assistant\"):\n",
    "            with st.spinner(\"Searching document and generating response...\"):\n",
    "                # Get context and response\n",
    "                context = query_supabase(user_question, st.session_state.embedder, st.session_state.document_tag)\n",
    "                response = get_gemini_response(user_question, context)\n",
    "                \n",
    "                # Display and save response\n",
    "                st.write(response)\n",
    "                st.session_state.chat_history.append({\"role\": \"assistant\", \"content\": response})\n",
    "                save_chat_history_to_database(st.session_state.document_tag, st.session_state.chat_history)\n",
    "else:\n",
    "    st.info(\"Please upload a PDF document or enter a website URL to start asking questions.\")\n",
    "\n",
    "# Footer\n",
    "st.markdown(\"---\")\n",
    "st.caption(\"Document & Web Q&A System using Supabase and Google Gemini\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
